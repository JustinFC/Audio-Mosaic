{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Mosaic Generator\n",
    "## Justin Chen, Nicolas Finkelstein, Kyle LaBrosse\n",
    "EECS352 -- Professor Pardo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import IPython, numpy as np, scipy as sp, matplotlib.pyplot as plt, matplotlib, sklearn, librosa, cmath,math, csv\n",
    "from IPython.display import Audio\n",
    "from sklearn.datasets import load_iris\n",
    "import random\n",
    "# This line makes sure your plots happen IN the webpage you're building, instead of in separate windows.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_audio(x, sr, figsize=(16,4)):\n",
    "    \"\"\"\n",
    "    A simple audio plotting function\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: np.ndarray\n",
    "        Audio signal to plot\n",
    "    sr: int\n",
    "        Sample rate\n",
    "    figsize: tuple\n",
    "        A duple representing the figure size (xdim,ydim)\n",
    "    \"\"\"\n",
    "    length = float(x.shape[0]) / sr\n",
    "    t = np.linspace(0,length,x.shape[0])\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(t, x)\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(x,window_len=11,window='hanning'):\n",
    "    \"\"\"smooth the data using a window with requested size.\n",
    "    \n",
    "    This method is based on the convolution of a scaled window with the signal.\n",
    "    The signal is prepared by introducing reflected copies of the signal \n",
    "    (with the window size) in both ends so that transient parts are minimized\n",
    "    in the begining and end part of the output signal.\n",
    "    \n",
    "    input:\n",
    "        x: the input signal \n",
    "        window_len: the dimension of the smoothing window; should be an odd integer\n",
    "        window: the type of window from 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\n",
    "            flat window will produce a moving average smoothing.\n",
    "\n",
    "    output:\n",
    "        the smoothed signal\n",
    "        \n",
    "    example:\n",
    "\n",
    "    t=linspace(-2,2,0.1)\n",
    "    x=sin(t)+randn(len(t))*0.1\n",
    "    y=smooth(x)\n",
    "    \n",
    "    see also: \n",
    "    \n",
    "    numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman, numpy.convolve\n",
    "    scipy.signal.lfilter\n",
    " \n",
    "    TODO: the window parameter could be the window itself if an array instead of a string\n",
    "    NOTE: length(output) != length(input), to correct this: return y[(window_len/2-1):-(window_len/2)] instead of just y.\n",
    "    \"\"\"\n",
    "\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError(\"smooth only accepts 1 dimension arrays.\")\n",
    "\n",
    "    if x.size < window_len:\n",
    "        raise ValueError(\"Input vector needs to be bigger than window size.\")\n",
    "\n",
    "\n",
    "    if window_len<3:\n",
    "        return x\n",
    "\n",
    "\n",
    "    if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n",
    "        raise ValueError(\"Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\")\n",
    "\n",
    "\n",
    "    s=np.r_[x[window_len-1:0:-1],x,x[-2:-window_len-1:-1]]\n",
    "    #print(len(s))\n",
    "    if window == 'flat': #moving average\n",
    "        w=np.ones(window_len,'d')\n",
    "    else:\n",
    "        w=eval('np.'+window+'(window_len)')\n",
    "\n",
    "    y=np.convolve(w/w.sum(),s,mode='valid')\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_filenames(path_to_esc50_csv):\n",
    "    \n",
    "    '''\n",
    "    Collect file names for training and testing set from `./dataset/meta/esc50.csv`. \n",
    "\n",
    "    Input Parameters\n",
    "    ----------------\n",
    "    path_to_esc50_csv: a string indicating a path to esc50.csv in ESC50 dataset.\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    ----------------\n",
    "    filenames: a dictionary containing file names of source set. \n",
    "                    Its keys are each class name: 'sneezing', 'snoring' \n",
    "    '''\n",
    "    filenames = {'laughing':[]}\n",
    "    \n",
    "    with open(path_to_esc50_csv) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            if row['category'] == 'laughing':\n",
    "                filenames['laughing'].append(row['filename'])\n",
    "                \n",
    "    \n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(signal, sr):\n",
    "    # EXTRACT PITCH and other features\n",
    "    # \n",
    "    mfcc = librosa.feature.mfcc(signal, sr=sr)\n",
    "    mfcc_mean = []\n",
    "    mfcc_delta_mean = []\n",
    "    \n",
    "    for val in mfcc:\n",
    "        mfcc_mean.append(np.mean(val))\n",
    "        mfcc_delta_mean.append(np.mean(np.diff(val)))\n",
    "    \n",
    "    mfcc_mean = np.array(mfcc_mean)\n",
    "    mfcc_delta_mean = np.array(mfcc_delta_mean)\n",
    "    feature_vector = np.concatenate((mfcc_mean, mfcc_delta_mean))\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(data_X, data_Y, query_X, dist_measure, k):\n",
    "    \"\"\"\n",
    "    Takes a data set of examples encoded as feature vectors, along with the label for each example in the data. \n",
    "    It also takes in a set of queries, for which we want to know the labels. It finds the distance from each \n",
    "    query_X to each example in data_X. It returns a label for each example in query_X by picking the most \n",
    "    popular label from the k nearest neighbors in data_X. Distance is determined by the selected distance metric.\n",
    "    \n",
    "    Input Parameters\n",
    "    ----------------\n",
    "    data_X: a 2-D numpy array with a shape of (the number of examples in the data, the number of features).\n",
    "    data_Y: a 1-D numpy array containing integer labels for the examples in data_X. \n",
    "            The labels should be encoded as integer values BEFORE they are passed this function.\n",
    "            (E.g., [class1, class1, class2, class1] ==> [0, 0, 1, 0])\n",
    "    query_X: a 2-D numpy array with a shape of (the number of query examples, the number of features). \n",
    "            Note, the query_X must have the same number of features, in the same order as the data_X \n",
    "    dist_measure: a string determining which distance measure to use. ('euclidean' or 'cosine')\n",
    "    k: the number of nearest neighbors in the data to consider, when labeling a query\n",
    "    \n",
    "    Returns\n",
    "    ----------------\n",
    "    query_Y: a 1-D numpy array of integer values referring to predicted labels for the set of queries\n",
    "    \"\"\"\n",
    "    query_Y = []\n",
    "    \n",
    "    for i in range(0, np.size(query_X, 0)):\n",
    "        dic = {}\n",
    "        for j in range (0, np.size(data_X, 0)):\n",
    "            label = data_Y[j]\n",
    "            if label not in dic:\n",
    "                dic[label] = []\n",
    "            if dist_measure == 'cosine':\n",
    "                distance = sp.spatial.distance.cosine(query_X[i], data_X[j])\n",
    "            elif dist_measure == 'euclidean':\n",
    "                distance = sp.spatial.distance.euclidean(query_X[i], data_X[j])\n",
    "            dic[label].append(distance)\n",
    "            \n",
    "        for key in dic:\n",
    "            dic[key].sort()\n",
    "        \n",
    "        closest_Labels = []\n",
    "        \n",
    "        for i in range (0, k):\n",
    "            minLabel = None\n",
    "            minVal = None\n",
    "            for key in dic:\n",
    "                if not dic[key]: continue\n",
    "                if minLabel is None: minLabel = key\n",
    "                if minVal is None: minVal = dic[key][0]\n",
    "                if dic[key][0] < minVal:\n",
    "                    minLabel = key\n",
    "                    minVal = dic[key][0]\n",
    "            closest_Labels.append(minLabel)\n",
    "            dic[minLabel].pop(0)\n",
    "        \n",
    "        Kclosest = max(set(closest_Labels), key=closest_Labels.count)\n",
    "        query_Y.append(Kclosest)\n",
    "    \n",
    "    return query_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_rescaling(train_X, test_X):\n",
    "    \n",
    "    '''\n",
    "\n",
    "    Input Parameters\n",
    "    ----------------\n",
    "    train_X: a 2-D numpy array with a shape of (the number of training examples, the number of features).\n",
    "    test_X: a 2-D numpy array with a shape of (the number of testing examples, the number of features).\n",
    "    \n",
    "    Returns\n",
    "    ----------------\n",
    "    train_rescaled_X: the rescaled version of train_X\n",
    "                    a 2-D numpy array with a shape of (the number of training examples, the number of features).\n",
    "    test_rescaled_X : the rescaled version of test_X\n",
    "                    a 2-D numpy array with a shape of (the number of testing examples, the number of features).\n",
    "    \n",
    "    '''\n",
    "    scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    \n",
    "    scaler.fit(train_X)\n",
    "    \n",
    "    train_rescaled_X = scaler.transform(train_X)\n",
    "    test_rescaled_X = scaler.transform(test_X)\n",
    "           \n",
    "    return train_rescaled_X, test_rescaled_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_source_target(index=24):\n",
    "    '''\n",
    "    Only used if using esc50 data. Index is number in range (0, 40) which contains the index\n",
    "    of the audio sample we wish to make our target/testing and exclude from training\n",
    "    '''\n",
    "    train_filenames = collect_filenames(\"./esc50/meta/esc50.csv\")\n",
    "\n",
    "    concat = np.array([])\n",
    "    count = 0\n",
    "    for key in train_filenames:\n",
    "        for file in train_filenames[key]:\n",
    "            if count == index:\n",
    "                continue\n",
    "            file = './esc50/audio/' + file\n",
    "            signal, sr = librosa.load(file, sr=None)\n",
    "            concat = np.concatenate([concat, signal])\n",
    "            count += 1\n",
    "\n",
    "    target = train_filenames[\"laughing\"][index]\n",
    "    file = \"./esc50/audio/\"+target\n",
    "    target, sr= librosa.load(file, sr=None)\n",
    "    # target = target[:len(target)//2]\n",
    "    return concat, target, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source, target, sr = parse_source_target(24)\n",
    "# Audio(target, rate=sr)\n",
    "\n",
    "source, sr = librosa.load('./audio/source.m4a', sr=None)\n",
    "target, sr = librosa.load('./audio/target.m4a', sr=None)\n",
    "Audio(target, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split source audio\n",
    "# source = concat\n",
    "\n",
    "def data_preparation(source, target, window_size=550):\n",
    "#     source = source[len(source)//2:]\n",
    "    training = []\n",
    "    testing = []\n",
    "\n",
    "    for i in range (0, len(source)-window_size, window_size): #only goes up to half of source\n",
    "        training.append(feature_extraction(source[i:i+window_size], sr))\n",
    "\n",
    "    for i in range (0, len(target)-window_size, window_size):\n",
    "        testing.append(feature_extraction(target[i:i+window_size], sr))\n",
    "    \n",
    "    training = np.array(training)\n",
    "    testing = np.array(testing)\n",
    "    \n",
    "    training, testing = feature_rescaling(training, testing)\n",
    "    label = range(0, len(training)) ## label holds which sample slice index was used\n",
    "    \n",
    "    return source, training, label, testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_source, training, label, testing = data_preparation(source, target, window_size=1200)\n",
    "\n",
    "guesses = knn(training, label, testing, 'cosine', 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing(x, window_size, window_type='hann'):\n",
    "    #preallocate output\n",
    "    signal = np.array([])\n",
    "    \n",
    "    #create window\n",
    "    if window_type == 'hann':\n",
    "        w = sp.signal.windows.hann(window_size)\n",
    "    elif window_type == 'tukey':\n",
    "        w = sp.signal.windows.tukey(window_size, alpha=.05)\n",
    "    \n",
    "    #apply hann window to signal\n",
    "    num_windows = math.floor(len(x)/window_size)\n",
    "    for i in range(0, num_windows):\n",
    "        signal = np.concatenate([signal, x[i*window_size:i*window_size+window_size]*w])\n",
    "        \n",
    "    return signal\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(guesses)\n",
    "sample = []\n",
    "window_size = 1200\n",
    "for i in range (0, len(source)-window_size, window_size):\n",
    "#     print(feature_extraction(source[i:window_size], sr))\n",
    "    sample.append(source[i:i+window_size])\n",
    "    \n",
    "signal = []\n",
    "for i in range(0,len(guesses)):\n",
    "    max_guess = np.amax(np.abs(sample[guesses[i]]))\n",
    "    max_target = np.amax(np.abs(target[i*window_size:i*window_size+window_size]))\n",
    "    signal = np.concatenate([signal,sample[guesses[i]]*max_target/max_guess])\n",
    "\n",
    "signal = np.array(signal)\n",
    "signal = smoothing(signal, window_size, window_type = 'tukey')\n",
    "#signal = smooth(signal, window_len = 11)\n",
    "plot_audio(signal, sr)\n",
    "Audio(signal, rate=sr)\n",
    "# librosa.output.write_wav('new_target.wav', signal, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
